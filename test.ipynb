{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# MyTrain"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## DataSetInitialization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "import logging\n",
    "from logging import getLogger\n",
    "from recbole.utils import init_logger, init_seed\n",
    "from mamba4poi import Mamba4POI\n",
    "from recbole.config import Config\n",
    "from utils import *\n",
    "from recbole.data.transform import construct_transform\n",
    "from recbole.utils import (\n",
    "    init_logger,\n",
    "    get_model,\n",
    "    get_trainer,\n",
    "    init_seed,\n",
    "    set_color,\n",
    "    get_flops,\n",
    "    get_environment,\n",
    ")\n",
    "import torch\n",
    "from myutils import * \n",
    "\n",
    "config = Config(model=Mamba4POI, config_file_list=['config.yaml'])\n",
    "dataset = create_dataset(config)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data,valid_data,test_data = data_preparation(config, dataset)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "k=50\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "\n",
    "num_user=(dataset.inter_feat[dataset.uid_field].max()+1).astype(int)\n",
    "num_category=(dataset.item_feat[\"venue_category_id\"].max()+1).astype(int)\n",
    "num_POI=(dataset.item_feat[\"venue_id\"].max()+1).astype(int)\n",
    "\n",
    "Ms=torch.zeros((num_user,num_category),dtype=torch.float32)\n",
    "_,M0,_,_=counting4all(train_data._dataset,device)\n",
    "\n",
    "Ec=torch.zeros((num_category,k),dtype=torch.float32)\n",
    "ESu=torch.zeros((num_user,k),dtype=torch.float32)\n",
    "\n",
    "itemX=(dataset.item_feat[\"longitude\"]).to_numpy()\n",
    "itemY=(dataset.item_feat[\"latitude\"]).to_numpy()\n",
    "itemC=(dataset.item_feat[\"venue_category_id\"]).to_numpy()\n",
    "Locations=torch.tensor(np.stack((itemX,itemY),axis=-1),dtype=torch.double)\n",
    "\n",
    "\n",
    "row_nonzero_mask=M0.sum(1)>0\n",
    "col_nonzero_mask=M0.sum(0)>0\n",
    "nonzero_M0=M0[row_nonzero_mask][:,col_nonzero_mask]\n",
    "\n",
    "E0u,S0,E0c=UC_SVD(nonzero_M0,k)\n",
    "\n",
    "Ec[col_nonzero_mask]=E0c\n",
    "\n",
    "EPOI=torch.einsum('ij,ik->ijk',Ec[itemC],Locations)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "for batch in train_data:\n",
    "    userids,item_seqs,Xs,Ys,Cs,labels,times,negs=batch\n",
    "    break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "Ms=accumulate_category(Cs,userids,Ms)\n",
    "s_row_nonzero_mask=Ms.sum(1)>0\n",
    "s_col_nonzero_mask=Ms.sum(0)>0\n",
    "nonzero_Ms=Ms[s_row_nonzero_mask][:,s_col_nonzero_mask]\n",
    "\n",
    "row,col=nonzero_Ms.shape\n",
    "size = max(row, col)  \n",
    "expanded_Ms = torch.zeros(size, size)\n",
    "expanded_Ms[:row,:col]=nonzero_Ms\n",
    "if row==size:\n",
    "    expanded_Ms[:,col:]=torch.eye(row,row - col) * 1e5\n",
    "else:\n",
    "    expanded_Ms[row:,:]=torch.eye(col-row,col)*1e5\n",
    "\n",
    "Esu,Ss,Esc=UC_SVD(expanded_Ms,k)\n",
    "\n",
    "Estu=Ec[s_col_nonzero_mask]@Esc.T@Esu\n",
    "ESu[s_row_nonzero_mask]=Estu[:row]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "location=np.stack((Xs,Ys),axis=-1)\n",
    "El=torch.tensor(location,dtype=torch.double)\n",
    "Ep=torch.einsum('usj,usk->usjk',Ec[Cs],El)#numuserXnumseqXcategoryfeatureXLocationfeature\n",
    "S=torch.einsum('ut,usjk->ustjk',ESu[userids[:,0]],Ep)#numuserXnumseqXcategoryfeatureXLocationfeatureXUserFeat\n",
    "labels=torch.tensor(np.array(labels))\n",
    "valid_S=S[(labels==1)|(labels==2)].reshape(S.shape[0],-1,S.shape[2],S.shape[3],S.shape[4])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "Times=torch.tensor(times,dtype=torch.double)\n",
    "dT=(Times[:,1:]-Times[:,:-1]).unsqueeze(2).unsqueeze(2).unsqueeze(2)\n",
    "S1=valid_S[:,1:]/dT\n",
    "S2=valid_S[:,:-1]/dT\n",
    "\n",
    "shape = S1.shape\n",
    "\n",
    "new_shape = (shape[0], shape[1] * 2, shape[2], shape[3], shape[4])\n",
    "\n",
    "Ss = torch.zeros(new_shape,dtype=torch.double)\n",
    "Ss[:,0::2]=S2\n",
    "Ss[:,1::2]=S1\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import gc\n",
    "torch.set_printoptions(profile=\"full\")\n",
    "for obj in gc.get_objects():\n",
    "    if torch.is_tensor(obj) and obj.is_cuda:\n",
    "        print(type(obj), obj.size())\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# HyperGraphModel"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### notes:\n",
    "We use a **HyperGraph Neural Network** to model the complex interactions among users, categories, and POIs, calculating the weight of hyperedges to reflect users' **interests in both categories and POIs**.<br>\n",
    "Unlike our sequential prediction model, which predict user interest in the dynamic time sequence, the HGNN addresses the issue of dissimilar interests in the same POI caused by users' varied **breadth of interests** despite having equal interaction levels,by modeling static user profiles.The **distinction** between the two modules is determined by their approach to **utilizing interactions**: ***dynamically or statically***<br>\n",
    "#### Nodes defination:\n",
    "We detatch category from POI features to modeling its latent<br> relation between both POI and user<br><br>\n",
    "**User**:       10dims for 3 features<br>\n",
    "*num_poi_interaction*:int,*num_category_interaction*:int<br>\n",
    "***active_area:tensor***(4,2) <--training target<br><br>\n",
    "**Category**:   1dim for 1 feature<br>\n",
    "*num_poi*:int,sigma:float(2,),center:float(2,)<br>\n",
    "***using poi geographic distribution feature to reflect the feature of category***<br>\n",
    "<br><br>\n",
    "**POI**:        2dims for 1feature<br>\n",
    "*location*:tensor(2,)<br>\n",
    "\n",
    "             \n",
    "\n",
    "#### Weight updating policy:\n",
    "Our Hyperedge connected with three varied type nodes,whose weight can't be fully normalized as scalar,are designed with vector weight.Using attention mechanism to reflect the diffrent impact from nodes.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from HGNN import *\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "torch.cuda.empty_cache()\n",
    "POI_interaction_matrix,category_interaction_matrix,category_ids_counts,unique_IM=counting4all(dataset,device)\n",
    "itemX = torch.tensor(dataset.item_feat[\"longtitude\"], dtype=torch.float32)\n",
    "itemY = torch.tensor(dataset.item_feat[\"latitude\"], dtype=torch.float32)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "center_X,center_Y,width,height=active_center_point(POI_interaction_matrix,unique_IM,itemX,itemY,device)\n",
    "catsim=category_interest_similarity(category_interaction_matrix,device)\n",
    "locsim=user_location_affinity_matrix(center_X,center_Y,width,height,device)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.set_printoptions(threshold=float('inf'))\n",
    "co=locsim+catsim\n",
    "co=(co>0).float()\n",
    "index_dict = {}\n",
    "\n",
    "for row_idx in range(co.size(0)):  # 遍历每一行\n",
    "    # 获取该行中所有值为1的列索引\n",
    "    col_indices = torch.nonzero(co[row_idx], as_tuple=True)[0].tolist()\n",
    "    index_dict[row_idx] = col_indices\n",
    "print(index_dict)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Venue Category Analysis"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 负采样优化\n",
    "空间融合基本方法：基于流行度采样之上，候选负样本缩小到用户活动范围内，目的是排除活动范围外流行的伪负样本\n",
    "活动范围：用户交互过的地点位置取最大外切矩形\n",
    "分层采样：依据交互次数排行 找到频繁访问区域作为生活区 最少访问区域作为危险区 其余部分作为探索区\n",
    "假设1：用户熟知生活区所有地点，因此没有访问的地点作为负样本\n",
    "假设2：危险区可能是用户真的不感兴趣的区域 也有可能是不方便涉足的区域，可以根据危险区和生活区的距离进行从分段处理控制采样比例 危险区采样比例根据其与生活区距离\n",
    "探索区域：融合类目分布进行采样\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "物品的特征可以这样观察 冷门-流行 丰富-稀有 再加上用户访问次数特征 构建出一个三维空间\n",
    "丰富稀有可以转化成 日常化程度 \n",
    "#### 假设1：相比更日常的地点人们对稀有的地点更感兴趣\n",
    "#### 假设2：用户更多访问的地方可能不是因为感兴趣而是因为日常生活需要\n",
    "#### 假设3：用户的频繁访问点是集中的，用户频繁访问区内的地点用户都熟知，未访问的地点大概率是不感兴趣\n",
    "#### 假设4：用户频繁访问区外的用户的偶尔访问点是集中的，用户偶尔访问区根据与频繁访问区的距离可以感知用户是对这块区域没兴趣还是不方便探索，这个距离是根据整体区域大小裁定的\n",
    "#### 假设5：除了偶尔访问区和频繁访问区，剩下的探索区是很未知的，\n",
    "#### 假设6：根据频繁访问区每个类目 用户访问占区域内总数比例 粗略推断用户对类目的兴趣 把这个权重作用在探索区\n",
    "#### 策略1：根据全体交互item划分出总区域\n",
    "#### 策略2：根据分位数划分出频繁访问item，在划分出频繁访问去地理包络，记录包络中心点位置，包络中未访问的项目直接划分成候选负样本\n",
    "#### 策略3：在频繁访问区地理包络外根据分位数划分出偶尔访问item，得到偶尔访问区地理包络，以中心位置到频繁访问区中心位置距离和总区域斜边两者倒数比来度量相近程度作为偶尔访问区采样权重\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "torch.cuda.empty_cache()\n",
    "\n",
    "POI_interaction_matrix,category_interaction_matrix,category_ids_counts=counting4all(dataset,device)\n",
    "\n",
    "reciprocalrarity=reciprocal_rarity(category_ids_counts)\n",
    "exprarity=exp_rarity(category_ids_counts,device)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 现象1：活跃用户和非活跃用户的交互分布特征相似 大多数访问的地点去的次数很少 只有少数地点去了很多次 这些地点反映用户兴趣"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import plotly.express as px\n",
    "import numpy as np\n",
    "\n",
    "# 假设 interaction_matrix 是给定的张量\n",
    "# Step 1: Identify the most active user (column with max interactions excluding the first row and column)\n",
    "active_user_idx = category_interaction_matrix[1:, 1:].sum(1).argmax()\n",
    "active_user = category_interaction_matrix[active_user_idx+1,:].cpu().numpy()\n",
    "\n",
    "# Step 2: Identify the least active user (column with min interactions excluding the first row and column)\n",
    "inactive_user_idx = category_interaction_matrix[1:, 1:].sum(1).argmin()\n",
    "inactive_user = category_interaction_matrix[inactive_user_idx+1,:].cpu().numpy()\n",
    "\n",
    "# 计算均值\n",
    "mean_interaction = category_interaction_matrix[1:,1:].mean(dtype=torch.float32)\n",
    "ordinary_user_idx = (abs(category_interaction_matrix[1:,1:] - mean_interaction)).argmin()\n",
    "ordinary_user=category_interaction_matrix[ordinary_user_idx+1,:].cpu().numpy()\n",
    "\n",
    "# Step 3: Filter out zeros\n",
    "active_user = active_user[active_user > 0]\n",
    "inactive_user = inactive_user[inactive_user > 0]\n",
    "ordinary_user=ordinary_user[ordinary_user>0]\n",
    "\n",
    "# Step 4: Create DataFrames for Plotly\n",
    "active_df = pd.DataFrame({\"Interactions\": active_user, \"User Type\": \"Active User\"})\n",
    "inactive_df = pd.DataFrame({\"Interactions\": inactive_user, \"User Type\": \"Inactive User\"})\n",
    "ordinary_df= pd.DataFrame({\"Interactions\": ordinary_user, \"User Type\": \"Ordinary User\"})\n",
    "\n",
    "# Combine both for easier plotting\n",
    "combined_df = pd.concat([active_df, inactive_df,ordinary_df])\n",
    "print(active_user.sum(),inactive_user.sum(),ordinary_user.sum())\n",
    "# Step 5: Plot histograms using Plotly\n",
    "\n",
    "fig = px.histogram(\n",
    "    combined_df,\n",
    "    x=\"Interactions\",\n",
    "    color=\"User Type\",\n",
    "    title=(\n",
    "      \"How can users' interests be effectively measured,<br>\"\n",
    "      \"while balancing the trade-off between interest breadth and depth,<br>\"\n",
    "      \"as illustrated by interaction quantities of <br>\"\n",
    "      \"User-Category and Category-POI (user-wise) or User-POI?\"),\n",
    "    labels={\"Interactions\": \"Number of Interactions\", \"User Type\": \"User Type\"},\n",
    "    nbins=100,  # Adjust the number of bins as needed\n",
    ")\n",
    "\n",
    "# Customize the title position and style\n",
    "fig.update_layout(\n",
    "    title={\n",
    "        \"y\": 0.7,  # Adjust vertical position (1.0 is top, 0.0 is bottom)\n",
    "        \"x\": 0.5,   # Adjust horizontal position (0.5 is centered)\n",
    "        \"xanchor\": \"center\",  # Anchor to center\n",
    "        \"yanchor\": \"top\",     # Anchor to top\n",
    "        \"font\": {\"size\": 16},  # Adjust font size\n",
    "    }\n",
    ")\n",
    "\n",
    "\n",
    "fig.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "问题：如何区分大量的少交互地点进行兴趣度量 实际上访问次数极多的地点不具有参考价值 它们可能是日常所需，不能很好地反映兴趣\n",
    "方案：把地点访问次数转化成类目访问次数 \n",
    "首先把userid-itemid交互序列转化成userid-categoryid交互序列进行归并 这样地分布更加平滑，访问总次数是一样的，只不过把大量只访问一次的地点归并成访问多次的类目。 \n",
    "这样我们就可以区分访问次数极少的地点 \n",
    "\n",
    "基于类目的兴趣度量：\n",
    "假设：偶尔访问类目和频繁访问类目不能反映兴趣\n",
    "定义用户兴趣类目：排除极端访问次数类目后的类目\n",
    "\n",
    "问题：用户对访问次数相同的类目兴趣度一致吗？应该怎么区分？\n",
    "方案：基于类目稀有度的加权，同样的访问次数，用户很可能对更稀有的类目感兴趣，引入类目稀有度矩阵\n",
    "\n",
    "后续模型修改：\n",
    "引入兴趣广度概念：不同用户以同样次数访问同个地点，反映的兴趣度是不同的，应该依赖用户访问总类目数量进行加权度量，得到更合理的用户-地点相似度"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "问题：如何把类目数量权重映射到稀有度上？\n",
    "直接映射：倒数加权，这样的得到的稀有度是突变的 我希望稀有度"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import plotly.graph_objects as go\n",
    "import torch\n",
    "\n",
    "\n",
    "reciprocal_rarity=reciprocalrarity.cpu()\n",
    "exp_rarity=exprarity.cpu()\n",
    "\n",
    "# 创建图表对象\n",
    "fig = go.Figure()\n",
    "\n",
    "# Reciprocal Rarity Histogram\n",
    "fig.add_trace(go.Histogram(\n",
    "    x=reciprocal_rarity[1:],\n",
    "    nbinsx=1000,\n",
    "    name=\"Reciprocal Rarity\",\n",
    "    marker_color=\"skyblue\",\n",
    "    histnorm='probability',\n",
    "    opacity=0.7,\n",
    "    showlegend=True  # 不显示图例\n",
    "))\n",
    "\n",
    "# Log Rarity Histogram\n",
    "fig.add_trace(go.Histogram(\n",
    "    x=exp_rarity[1:],\n",
    "    nbinsx=500,\n",
    "    name=\"Exp Rarity\",\n",
    "    marker_color=\"lightgreen\",\n",
    "    opacity=0.7,\n",
    "    histnorm='probability',\n",
    "    showlegend=True  # 不显示图例\n",
    "))\n",
    "\n",
    "# 计算方差和峰度\n",
    "reciprocal_var = torch.var(reciprocal_rarity[1:])\n",
    "reciprocal_kurt = kurtosis(reciprocal_rarity[1:])\n",
    "exp_var = torch.var(exp_rarity[1:])\n",
    "exp_kurt = kurtosis(exp_rarity[1:])\n",
    "\n",
    "\n",
    "# 添加外部文本框显示方差和峰度\n",
    "fig.add_annotation(\n",
    "    x=0.5,  # x轴位置，控制文本框水平位置\n",
    "    y=1.15,  # y轴位置，调整文本框在图表的外部\n",
    "    text=f\"Reciprocal Rarity\\nVar: {reciprocal_var:.4e}, Kurtosis: {reciprocal_kurt:.4e}\",\n",
    "    showarrow=False,\n",
    "    font=dict(size=12, color=\"black\"),\n",
    "    align=\"center\",\n",
    "    bgcolor=\"white\",\n",
    "    opacity=0.8,\n",
    "    xref=\"paper\",  # 使用 'paper' 坐标系，让文本框不受图表范围限制\n",
    "    yref=\"paper\"   # 使用 'paper' 坐标系，使文本框在图表外\n",
    ")\n",
    "\n",
    "fig.add_annotation(\n",
    "    x=0.5,  # x轴位置，控制文本框水平位置\n",
    "    y=1.05,  # y轴位置，调整文本框在图表的外部\n",
    "    text=f\"Exp Rarity\\nVar: {exp_var:.4e}, Kurtosis: {exp_kurt:.4e}\",\n",
    "    showarrow=False,\n",
    "    font=dict(size=12, color=\"black\"),\n",
    "    align=\"center\",\n",
    "    bgcolor=\"white\",\n",
    "    opacity=0.8,\n",
    "    xref=\"paper\",  # 使用 'paper' 坐标系，放置在图表外\n",
    "    yref=\"paper\"   # 使用 'paper' 坐标系，放置在图表外\n",
    ")\n",
    "\n",
    "# 更新布局\n",
    "fig.update_layout(\n",
    "    title=\"Histograms of Reciprocal and Log Rarity\",\n",
    "    xaxis_title=\"Value\",\n",
    "    yaxis_title=\"Frequency\",\n",
    "    title_font_size=20,\n",
    "    barmode=\"overlay\",  # 使用 \"overlay\" 叠加直方图，\"group\" 可并排显示\n",
    "    showlegend=True    # 隐藏图例\n",
    ")\n",
    "\n",
    "# 显示图表\n",
    "fig.show()#5.96"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "\n",
    "# 2. 将数据转为 PyTorch Tensor 并移动到 GPU 上\n",
    "venue_id = torch.tensor(venue_id, dtype=torch.long).cuda()  \n",
    "venue_category = torch.tensor(venue_category, dtype=torch.long).cuda()\n",
    "x = torch.tensor(x, dtype=torch.float32).cuda()  \n",
    "y = torch.tensor(y, dtype=torch.float32).cuda()\n",
    "\n",
    "# 3. 设置网格的尺寸（例如：每个网格大小为500x500）\n",
    "grid_width = 500  # 网格宽度\n",
    "grid_height = 500  # 网格高度\n",
    "\n",
    "# 4. 计算网格的数量（基于坐标的最大最小值）\n",
    "x_min, x_max = x.min(), x.max()\n",
    "y_min, y_max = y.min(), y.max()\n",
    "\n",
    "# 计算网格行列数\n",
    "num_x_grids = int((x_max - x_min) // grid_width) + 1\n",
    "num_y_grids = int((y_max - y_min) // grid_height) + 1\n",
    "\n",
    "\n",
    "# 5. 分批次处理\n",
    "batch_size = 10000  # 每批处理的样本数量\n",
    "num_batches = len(venue_id) // batch_size + 1  # 批次数量\n",
    "\n",
    "# 创建稠密张量来暂存更新，最后将其转换为稀疏张量\n",
    "venue_density_matrix = torch.zeros((num_y_grids, num_x_grids), dtype=torch.int32).cuda()\n",
    "category_density_matrix = torch.zeros((num_y_grids, num_x_grids, len(torch.unique(venue_category))), dtype=torch.int32).cuda()\n",
    "\n",
    "for batch_idx in range(num_batches):\n",
    "    # 计算每个批次的索引范围\n",
    "    start_idx = batch_idx * batch_size\n",
    "    end_idx = min((batch_idx + 1) * batch_size, len(venue_id))\n",
    "    \n",
    "    # 获取当前批次数据\n",
    "    batch_venue_id = venue_id[start_idx:end_idx]\n",
    "    batch_venue_category = venue_category[start_idx:end_idx]\n",
    "    batch_x = x[start_idx:end_idx]\n",
    "    batch_y = y[start_idx:end_idx]\n",
    "\n",
    "    # 计算每个数据点所属的网格\n",
    "    grid_x = ((batch_x - x_min) / grid_width).floor().long()  # 计算 x 对应的网格位置\n",
    "    grid_y = ((batch_y - y_min) / grid_height).floor().long()  # 计算 y 对应的网格位置\n",
    "\n",
    "    # 确保网格索引不超出边界\n",
    "    grid_x = torch.clamp(grid_x, 0, num_x_grids - 1)\n",
    "    grid_y = torch.clamp(grid_y, 0, num_y_grids - 1)\n",
    "\n",
    "    # 使用 scatter_add_ 更新稠密张量的计数\n",
    "    venue_density_matrix.index_put_((grid_y, grid_x), torch.ones(len(grid_y), dtype=torch.int32).cuda(), accumulate=True)\n",
    "    \n",
    "    # 更新类别密度矩阵\n",
    "    for i in range(len(batch_venue_category)):\n",
    "        category_density_matrix[grid_y[i], grid_x[i], batch_venue_category[i]] += 1\n",
    "\n",
    "# 7. 将稠密张量转换为稀疏张量\n",
    "venue_density_matrix_sparse = venue_density_matrix.to_sparse()\n",
    "category_density_matrix_sparse = category_density_matrix.to_sparse()\n",
    "\n",
    "# 8. 打印结果\n",
    "print(\"Venue Density Matrix Sparse:\")\n",
    "print(venue_density_matrix_sparse)\n",
    "print(\"Category Density Matrix Sparse:\")\n",
    "print(category_density_matrix_sparse)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import plotly.graph_objects as go\n",
    "import plotly.colors as pc\n",
    "import torch\n",
    "\n",
    "# 假设 venue_density_matrix_sparse 是一个 PyTorch tensor\n",
    "# 你可以根据你实际的场景替换这个变量\n",
    "venue_density_matrix_sparse = torch.rand(num_y_grids, num_x_grids)  # 模拟密度矩阵\n",
    "\n",
    "# 将 x_min 和 y_min 移回 CPU\n",
    "xmin = x_min.cpu().item()\n",
    "ymin = y_min.cpu().item()\n",
    "\n",
    "# 创建空的图形对象\n",
    "fig = go.Figure()\n",
    "\n",
    "# 定义颜色范围，这里选择一个颜色调色板，如蓝色渐变\n",
    "color_scale = 'Blues'  # 可以选择其他颜色范围，如 'Viridis', 'Cividis', 'Inferno'\n",
    "\n",
    "# 使用 PyTorch 的 max 和 min 函数\n",
    "max_density = torch.max(venue_density_matrix_sparse).item()  # 获取最大密度\n",
    "min_density = torch.min(venue_density_matrix_sparse).item()  # 获取最小密度\n",
    "normalized_density = (venue_density_matrix_sparse - min_density) / (max_density - min_density)  # 归一化\n",
    "\n",
    "# 添加网格图层：根据密度值使用颜色\n",
    "for i in range(num_y_grids):\n",
    "    for j in range(num_x_grids):\n",
    "        # 计算网格的边界\n",
    "        grid_x_min = xmin + j * grid_width\n",
    "        grid_x_max = grid_x_min + grid_width\n",
    "        grid_y_min = ymin + i * grid_height\n",
    "        grid_y_max = grid_y_min + grid_height\n",
    "        \n",
    "        # 根据网格的密度值计算颜色\n",
    "        density = normalized_density[i, j].item()  # 转换为 Python 数字\n",
    "        # 使用颜色渐变进行映射\n",
    "        fillcolor = pc.sequential.Blues[int(density * (len(pc.sequential.Blues) - 1))]\n",
    "\n",
    "        # 绘制矩形网格，设置透明度\n",
    "        fig.add_trace(go.Scatter(\n",
    "            x=[grid_x_min, grid_x_min, grid_x_max, grid_x_max, grid_x_min],\n",
    "            y=[grid_y_min, grid_y_max, grid_y_max, grid_y_min, grid_y_min],\n",
    "            fill='toself',\n",
    "            fillcolor=fillcolor,\n",
    "            line=dict(width=1, color='black'),  # 添加黑色网格线\n",
    "            mode='lines',\n",
    "            name='Grid',  # 给网格设置名字，用于筛选\n",
    "            visible=True  # 默认显示\n",
    "        ))\n",
    "\n",
    "# 创建一个调色板来表示 category_id 的颜色\n",
    "category_colors = pc.qualitative.Set1\n",
    "marker_colors = [category_colors[c % len(category_colors)] for c in venue_category.cpu().numpy()]\n",
    "\n",
    "# 添加数据点图层\n",
    "fig.add_trace(go.Scattergl(\n",
    "    x=x.cpu().numpy(),\n",
    "    y=y.cpu().numpy(),\n",
    "    mode='markers',\n",
    "    marker=dict(\n",
    "        size=5,\n",
    "        color=marker_colors,  # 使用 category_id 作为颜色\n",
    "        showscale=True  # 显示颜色条\n",
    "    ),\n",
    "    name='Data Points',  # 给数据点设置名字，用于筛选\n",
    "    visible=True  # 默认显示\n",
    "))\n",
    "\n",
    "# 设置布局\n",
    "fig.update_layout(\n",
    "    title=\"Venue Points with Density-based Coloring and Grid Lines\",\n",
    "    xaxis_title=\"Longitude\",\n",
    "    yaxis_title=\"Latitude\",\n",
    "    showlegend=True,\n",
    "    updatemenus=[\n",
    "        {\n",
    "            'buttons': [\n",
    "                {\n",
    "                    'args': [None, {'visible': [True, False]}],  # 隐藏数据点，只显示网格\n",
    "                    'label': 'Show Grid Only',\n",
    "                    'method': 'relayout'\n",
    "                },\n",
    "                {\n",
    "                    'args': [None, {'visible': [False, True]}],  # 隐藏网格，只显示数据点\n",
    "                    'label': 'Show Data Points Only',\n",
    "                    'method': 'relayout'\n",
    "                },\n",
    "                {\n",
    "                    'args': [None, {'visible': [True, True]}],  # 显示所有\n",
    "                    'label': 'Show Both',\n",
    "                    'method': 'relayout'\n",
    "                }\n",
    "            ],\n",
    "            'direction': 'down',\n",
    "            'showactive': True,\n",
    "            'x': 0.17,\n",
    "            'xanchor': 'left',\n",
    "            'y': 1.15,\n",
    "            'yanchor': 'top'\n",
    "        }\n",
    "    ]\n",
    ")\n",
    "\n",
    "# 在浏览器中显示\n",
    "fig.show(renderer=\"browser\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "地点：偏远中心稀有丰富\n",
    "街区：偏远中心密集稀疏\n",
    "用500m*500m作为基本地理块进行聚合\n",
    "街区密度阈值 街区偏远度阈值\n",
    "中心和密集相关\n",
    "丰富地点一定是便利店之类的常用地点\n",
    "重点是稀有地点区分\n",
    "偏远但密集的是乡镇 偏远密集街区的丰富地点是便民设施 稀有地点大概率是学校 政府等大场地 小概率是景点\n",
    "偏远稀疏的是无人区 偏远稀疏街区的丰富地点可能就是便利店 但是稀有地点很可能是景点\n",
    "中心稀疏是城郊 景点的概率较大\n",
    "中心密集的是市中心 这是推荐的重点"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(dataset.item_feat)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import utils\n",
    "importlib.reload(utils)\n",
    "from utils import *\n",
    "# dataset splitting\n",
    "train_data,_,_ = data_preparation(config, dataset)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(dataset.inter_feat)\n",
    "for batch_idx, interaction in enumerate(train_data):\n",
    "    print(interaction)\n",
    "    break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "import importlib\n",
    "import mamba4poi\n",
    "import utils\n",
    "importlib.reload(mamba4poi)\n",
    "importlib.reload(utils)\n",
    "from utils import *\n",
    "from mamba4poi import Mamba4POI\n",
    "import os\n",
    "# 设置 CUDA_LAUNCH_BLOCKING 为 1\n",
    "os.environ['CUDA_LAUNCH_BLOCKING'] = '1'\n",
    "if __name__ == '__main__':\n",
    "    config = Config(model=Mamba4POI, config_file_list=['config.yaml'])\n",
    "    init_seed(config['seed'], config['reproducibility'])\n",
    "    \n",
    "    # logger initialization\n",
    "    init_logger(config)\n",
    "    logger = getLogger()\n",
    "    logger.info(sys.argv)\n",
    "    logger.info(config)\n",
    "\n",
    "    logger.info(dataset)\n",
    "\n",
    "    # model loading and initialization\n",
    "    init_seed(config[\"seed\"] + config[\"local_rank\"], config[\"reproducibility\"])\n",
    "    model = Mamba4POI(config, train_data.dataset).to(config['device'])\n",
    "    logger.info(model)\n",
    "    \n",
    "    transform = construct_transform(config)\n",
    "    flops = get_flops(model, dataset, config[\"device\"], logger, transform)\n",
    "    logger.info(set_color(\"FLOPs\", \"blue\") + f\": {flops}\")\n",
    "\n",
    "    # trainer loading and initialization\n",
    "    trainer = Trainer(config, model)\n",
    "\n",
    "    best_valid_score, best_valid_result = trainer.fit(\n",
    "    train_data,\n",
    "    valid_data,  # 可以保留验证数据集\n",
    "    verbose=True,    # 保留详细信息，打印结果\n",
    "    saved=True,      # 根据需要决定是否保存模型参数\n",
    "    show_progress=True,  # 不显示进度条\n",
    "    callback_fn=None  # 如果不需要回调函数，可以设置为 None\n",
    ")\n",
    "\n",
    "\n",
    "\n",
    "    # model evaluation\n",
    "    test_result = trainer.evaluate(\n",
    "        test_data, show_progress=config[\"show_progress\"]\n",
    "    )\n",
    "    \n",
    "    environment_tb = get_environment(config)\n",
    "    logger.info(\n",
    "        \"The running environment of this training is as follows:\\n\"\n",
    "        + environment_tb.draw()\n",
    "    )\n",
    "\n",
    "    logger.info(set_color(\"best valid \", \"yellow\") + f\": {best_valid_result}\")\n",
    "    logger.info(set_color(\"test result\", \"yellow\") + f\": {test_result}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Pytorch",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.20"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
